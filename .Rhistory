#So first we create a count of bing sentiment words that occur a lot in the corpus.
bing = get_sentiments("bing")
bing_word_counts <- textdf %>%
unnest_tokens(word, text) %>%
inner_join(bing) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
bing_word_counts
###visualize
bing_word_counts %>%
filter(n > 3) %>%
mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_bar(stat = "identity") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
ylab("Contribution to sentiment")
#####wordclouds
require(wordcloud)
# build wordcloud of commonest tokens
textdf %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, scale=c(5,0.5), max.words=100, random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2")))
nrc = get_sentiments("nrc")
senti.nrc = textdf %>%
mutate(linenumber = row_number()) %>%
ungroup() %>%
# word-tokenize & merge nrc sentiment words
unnest_tokens(word, text) %>%
inner_join(get_sentiments("nrc")) %>%
count(sentiment, index = linenumber %/% 1, sort = FALSE) %>%  # %/% gives quotient
mutate(method = "nrc")
senti.nrc %>% tail()
senti.nrc$sentiment
a = data.frame(senti.nrc %>% spread(sentiment, n, fill = 0))
head(a)
###what joyful words most occurred in the corpus.
bitcoin_joy = textdf %>%
unnest_tokens(word, text) %>%
inner_join(nrc) %>%
filter(sentiment == "joy") %>%
count(word, sort = TRUE)
bitcoin_joy %>% head()
###what fearful words most occurred in the corpus.
bitcoin_fear = textdf %>%
unnest_tokens(word, text) %>%
inner_join(nrc) %>%
filter(sentiment == "fear") %>%
count(word, sort = TRUE)
bitcoin_fear %>% head()
#############  Sentiment analysis on Live streaming data from twitter.
library(ROAuth)
consumer_key = "e2zbHdO72bF3KKXPJ8kIJ8P3C"
consumer_secret = "fjb2L0jdDzeJxvodDsEQs7UVYDJty5eWfwtIDTcMk8tZ8SyBcd"
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
my_oauth <- OAuthFactory$new(consumerKey=consumer_key,
consumerSecret=consumer_secret, requestURL=requestURL,
accessURL=accessURL, authURL=authURL)
my_oauth$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
library(streamR)
filterStream(file.name="crypto.json", track="cryptocurrency",
timeout=300, oauth=my_oauth)
crypto_tweets <- parseTweets("crypto.json")
colnames(crypto_tweets)
View(crypto_tweets)
filterStream(file.name="btc.json", track="bitcoin",
timeout=300, oauth=my_oauth)
bitcoin_tweets <- parseTweets("btc.json")
filterStream(file.name="ethereum.json", track="ethereum",
timeout=100, oauth=my_oauth)
ethereum_tweets <- parseTweets("ethereum.json")
### hashtags in cryptocurrency tweets
crypto_hashtags = str_extract_all(crypto_tweets$text, "#\\w+")
class(crypto_hashtags)
crypto_hashtags =  unlist(crypto_hashtags, recursive = TRUE)
crypto_tags_freq = table(crypto_hashtags)
crypto_tags_freq <- sort(crypto_tags_freq,decreasing = TRUE)
head(crypto_tags_freq,20)  #top 20
####hashtags in bitcoin
btc_hashtags = str_extract_all(bitcoin_tweets$text, "#\\w+")
class(btc_hashtags)
btc_hashtags =  unlist(btc_hashtags, recursive = TRUE)
btc_tags_freq = table(btc_hashtags)
btc_tags_freq <- sort(btc_tags_freq,decreasing = TRUE)
head(btc_tags_freq,20)  #top 20
### wordcloud of bitcoin tweets
require(tm)
require(RColorBrewer)
wordcloud(names(btc_tags_freq), btc_tags_freq, random.order=FALSE, colors = brewer.pal(12, "Paired"))
## wordcloud of cryptocurrency tweets
require(tm)
require(RColorBrewer)
### wordcloud
wordcloud(names(crypto_tags_freq), crypto_tags_freq, random.order=FALSE, colors = brewer.pal(12, "Paired"))
require(sentimentr)
mytext <- bitcoin_tweets$text
mytext <- get_sentences(mytext)
df <- sentiment_by(mytext)
class(df)
df$sentiment[df$ave_sentiment < 0 & df$ave_sentiment >= -1 ] <- -1
df$sentiment[df$ave_sentiment  > 0 & df$ave_sentiment <= 1] <- 1
df$sentiment[df$ave_sentiment == 0] <- 0
z <- bitcoin_tweets$text
y <- df$sentiment
data <- data.frame(y,z)
head(data,10)
# if (!require(RTextTools)) install.packages("RTextTools")
library("tm")
library("RTextTools")
library(magrittr)
colnames(data) = c('sentiment','text')  # Rename variables
head(data)
set.seed(12345)                 # To fix the sample
samp_id = sample(1:nrow(data),
round(nrow(data)*.70), # 70% records 4 training
replace = F)
train = data[samp_id,]      # 70% of training sample
test = data[-samp_id,]      # remaining 30% of training sample
dim(test) ; dim(train)
train.data = rbind(train,test)    # join the data sets
text = train.data$text
text <- as.character(text)
text = removePunctuation(text)    # remove punctuation marks
text = removeNumbers(text)        # remove numbers
text = stripWhitespace(text)      # remove blank space
cor = Corpus(VectorSource(text))  # Create text corpus
dtm = DocumentTermMatrix(cor,
control = list(weighting =
function(x)
weightTfIdf(x, normalize = F)))
training_codes = train.data$sentiment       # Coded labels
# dim(dtm)    # 7086 x 2118
dtm[1:6, 1:6]
## building containers around the data samples
container = create_container(dtm,
t(training_codes),
trainSize = 1:nrow(train),
testSize =
(nrow(train)+1): nrow(train.data),
virgin=TRUE)
## building containers around the data samples
container = create_container(dtm,
t(training_codes),
trainSize = 1:nrow(train),
testSize =
(nrow(train)+1): nrow(train.data),
virgin=TRUE)
## building containers around the data samples
container = create_container(dtm,
t(training_codes),
trainSize = 1:(nrow(train)),
testSize =
(nrow(train)+1): nrow(train.data),
virgin=TRUE)
nrow(train)
nrow(train.data)
# create the analytics object
analytics = create_analytics(container, results)
class(analytics)
str(analytics)
# view obj attribs
head(analytics@algorithm_summary)
class(container)
system.time({
models <- train_models(container,
algorithms = c("MAXENT", "SVM"))
#                                      "GLMNET", "SLDA", "TREE",
#                                      "BAGGING", "BOOSTING",
#                                      "RF"))
})   # ~ 4 secs
results = classify_models(container, models)
str(results)
# clubbing text & label with prediction
text_n_results = data.frame(test, results)
# Just check one algorithm
models = train_models(container, algorithms=c("MAXENT"))
results = classify_models(container, models)
head(results)
# create results df with more detail
out = data.frame(model_sentiment = results$MAXENTROPY_LABEL,
model_prob = results$MAXENTROPY_PROB,
actual_sentiment = train.data$sentiment[(nrow(train)+1):nrow(train.data)])
data.test = crypto_tweets$text
data.test <- data.frame(data.test)
colnames(data.test) = 'text'
set.seed(12345)
# randomly Selecting only 500 rows for demonstration purpose
data.test1 = data.test[sample(1:nrow(data.test),500),]
text = data.test1
text <- as.character(text)
text = removePunctuation(text)
text = removeNumbers(text)
text = stripWhitespace(text)
cor = Corpus(VectorSource(text))
dtm.test = DocumentTermMatrix(cor,
control = list(weighting =
function(x)
weightTfIdf(x, normalize = F)))
nrow(dtm.test)
row.names(dtm.test) = (nrow(dtm)+1):(nrow(dtm)+nrow(dtm.test))
dtm.f = c(dtm, dtm.test)
training_codes.f = c(training_codes,rep(NA,length(data.test1)))
## building containers around the data samples
container.f = create_container(dtm.f,t(training_codes.f),trainSize=1:(nrow(dtm.test)), testSize = (nrow(dtm)+1):(nrow(dtm)+nrow(dtm.test)), virgin = F)
container.f = create_container(dtm.f,t(training_codes.f),trainSize=1:(nrow(dtm.test)), testSize = (nrow(dtm)+1):(nrow(dtm)+nrow(dtm.test)), virgin = F)
c <- prop.table(table(out$model_sentiment))
plot(c,type = "h", col = "red", lwd = 10,main = "proportion of tweets in random sample",xlab = "sentiment")
require(sentimentr)
mytext1 <- crypto_tweets$text
mytext1<- get_sentences(mytext1)
df1 <- sentiment_by(mytext1)
class(df1)
df1$sentiment[df1$ave_sentiment < 0 & df1$ave_sentiment >= -1 ] <- -1
df1$sentiment[df1$ave_sentiment  > 0 & df1$ave_sentiment <= 1] <- 1
df1$sentiment[df1$ave_sentiment == 0] <- 0
c<- prop.table(table(df1$sentiment))
plot(c,type = "h", col = "red", lwd = 10,main = "proportion of tweets in actual population",xlab = "sentiment")
# for bitcoin
filterStream(file.name="tweets_geo.json", locations=c(-125, 25, -66, 50),track = "bitcoin",
timeout=100, oauth=my_oauth)
tweets_geo_tweets <- parseTweets("tweets_geo.json")
###analysis of hashtags
tweets_geo_tweets_hashtags = str_extract_all(tweets_geo_tweets$text, "#\\w+")
# put tags in vector
tweets_geo_tweets_hashtags= unlist(tweets_geo_tweets_hashtags)
# calculate hashtag frequencies
tweets_geo_tweets_hashtags_freq = table(tweets_geo_tweets_hashtags)
tweets_geo_tweets_hashtags_freq<- sort(tweets_geo_tweets_hashtags_freq,decreasing = TRUE)
require(sentimentr)
mytext1 <- tweets_geo_tweets$text
mytext1<- get_sentences(mytext1)
df2 <- sentiment_by(mytext1)
class(df2)
df2$sentiment[df2$ave_sentiment < 0 & df2$ave_sentiment >= -1 ] <- -1
df2$sentiment[df2$ave_sentiment  > 0 & df2$ave_sentiment <= 1] <- 1
df2$sentiment[df2$ave_sentiment == 0] <- 0
c<- prop.table(table(df2$sentiment))
plot(c,type = "h", col = "red", lwd = 10,main = "proportion of sentiments in tweets in US",xlab = "sentiment")
#for bitcoin
filterStream(file.name="tweets_geo_india.json", locations=c(70, 10, 100, 35),track = "bitcoin",
timeout=100, oauth=my_oauth)
tweets_geo_tweets <- parseTweets("tweets_geo_india.json")
###analysis of hashtags
tweets_geo_tweets_hashtags = str_extract_all(tweets_geo_tweets$text, "#\\w+")
# put tags in vector
tweets_geo_tweets_hashtags= unlist(tweets_geo_tweets_hashtags)
# calculate hashtag frequencies
tweets_geo_tweets_hashtags_freq = table(tweets_geo_tweets_hashtags)
tweets_geo_tweets_hashtags_freq<- sort(tweets_geo_tweets_hashtags_freq,decreasing = TRUE)
#for bitcoin
filterStream(file.name="tweets_geo_india.json", locations=c(70, 10, 100, 35),track = "bitcoin",
timeout=100, oauth=my_oauth)
tweets_geo_tweets <- parseTweets("tweets_geo_india.json")
###analysis of hashtags
tweets_geo_tweets_hashtags = str_extract_all(tweets_geo_tweets$text, "#\\w+")
# put tags in vector
tweets_geo_tweets_hashtags= unlist(tweets_geo_tweets_hashtags)
# calculate hashtag frequencies
tweets_geo_tweets_hashtags_freq = table(tweets_geo_tweets_hashtags)
tweets_geo_tweets_hashtags_freq<- sort(tweets_geo_tweets_hashtags_freq,decreasing = TRUE)
require(sentimentr)
mytext1 <- tweets_geo_tweets$text
mytext1<- get_sentences(mytext1)
df2 <- sentiment_by(mytext1)
class(df2)
df2$sentiment[df2$ave_sentiment < 0 & df2$ave_sentiment >= -1 ] <- -1
df2$sentiment[df2$ave_sentiment  > 0 & df2$ave_sentiment <= 1] <- 1
df2$sentiment[df2$ave_sentiment == 0] <- 0
c<- prop.table(table(df2$sentiment))
plot(c,type = "h", col = "red", lwd = 10,main = "proportion of sentiments in tweets in India",xlab = "sentiment")
#for bitcoin
filterStream(file.name="tweets_geo_japan.json", locations=c(130, 30, 145, 45),track = "bitcoin",
timeout=100, oauth=my_oauth)
tweets_geo_tweets <- parseTweets("tweets_geo_japan.json")
###analysis of hashtags
tweets_geo_tweets_hashtags = str_extract_all(tweets_geo_tweets$text, "#\\w+")
# put tags in vector
tweets_geo_tweets_hashtags= unlist(tweets_geo_tweets_hashtags)
# calculate hashtag frequencies
tweets_geo_tweets_hashtags_freq = table(tweets_geo_tweets_hashtags)
tweets_geo_tweets_hashtags_freq<- sort(tweets_geo_tweets_hashtags_freq,decreasing = TRUE)
require(sentimentr)
mytext1 <- tweets_geo_tweets$text
mytext1<- get_sentences(mytext1)
df2 <- sentiment_by(mytext1)
class(df2)
df2$sentiment[df2$ave_sentiment < 0 & df2$ave_sentiment >= -1 ] <- -1
df2$sentiment[df2$ave_sentiment  > 0 & df2$ave_sentiment <= 1] <- 1
df2$sentiment[df2$ave_sentiment == 0] <- 0
c<- prop.table(table(df2$sentiment))
plot(c,type = "h", col = "red", lwd = 10,main = "proportion of sentiments in tweets in japan",xlab = "sentiment")
tweets_news
set.seed(12345)                 # To fix the sample
samp_id = sample(1:nrow(data),
round(nrow(data)*.70), # 70% records 4 training
replace = F)
train = data[samp_id,]      # 70% of training sample
test = data[-samp_id,]      # remaining 30% of training sample
dim(test) ; dim(train)
train.data = rbind(train,test)    # join the data sets
text = train.data$text
text <- as.character(text)
text = removePunctuation(text)    # remove punctuation marks
text = removeNumbers(text)        # remove numbers
text = stripWhitespace(text)      # remove blank space
cor = Corpus(VectorSource(text))  # Create text corpus
dtm = DocumentTermMatrix(cor,
control = list(weighting =
function(x)
weightTfIdf(x, normalize = F)))
training_codes = train.data$sentiment       # Coded labels
# dim(dtm)    # 7086 x 2118
dtm[1:6, 1:6]
## building containers around the data samples
container = create_container(dtm,
t(training_codes),
trainSize = 1:(nrow(train)),
testSize =
(nrow(train)+1): nrow(train.data),
virgin=TRUE)
## building containers around the data samples
container = create_container(dtm,
t(training_codes),
trainSize = 1:(nrow(train)),
testSize =
(nrow(train)+1): nrow(train.data),
virgin=TRUE)
## building containers around the data samples
container = create_container(dtm,
t(training_codes),
trainSize = 1:(nrow(train)),
testSize =
(nrow(train)+1): nrow(train.data),
virgin=TRUE)
## building containers around the data samples
container = create_container(dtm,
t(training_codes),
trainSize = 1:(nrow(train)),
testSize =
(nrow(train)+1): nrow(train.data),
virgin=TRUE)
## building containers around the data samples
container = create_container(dtm,
t(training_codes),
trainSize = 1:(nrow(train)),
testSize =
(nrow(train)+1): nrow(train.data),
virgin=TRUE)
## building containers around the data samples
container = create_container(dtm,
t(training_codes),
trainSize = 1:(nrow(train)),
testSize =
(nrow(train)+1): nrow(train.data),
virgin=TRUE)
## building containers around the data samples
container = create_container(dtm,
t(training_codes),
trainSize = 1:(nrow(train)),
testSize =
(nrow(train)+1): nrow(train.data),
virgin=TRUE)
## building containers around the data samples
container = create_container(dtm,
t(training_codes),
trainSize = 1:(nrow(train)),
testSize =
(nrow(train)+1): nrow(train.data),
virgin=TRUE)
## building containers around the data samples
container = create_container(dtm,
t(training_codes),
trainSize = 1:(nrow(train)),
testSize =
(nrow(train)+1): nrow(train.data),
virgin=TRUE)
train.data = rbind(train,test)    # join the data sets
text = train.data$text
text <- as.character(text)
text = removePunctuation(text)    # remove punctuation marks
text = removeNumbers(text)        # remove numbers
text = stripWhitespace(text)      # remove blank space
cor = Corpus(VectorSource(text))  # Create text corpus
dtm = DocumentTermMatrix(cor,
control = list(weighting =
function(x)
weightTfIdf(x, normalize = F)))
training_codes = train.data$sentiment       # Coded labels
# dim(dtm)    # 7086 x 2118
dtm[1:6, 1:6]
## building containers around the data samples
container = create_container(dtm,
t(training_codes),
trainSize = 1:(nrow(train)),
testSize =
(nrow(train)+1): nrow(train.data),
virgin=TRUE)
container = create_container(dtm,
t(training_codes),
trainSize = 1:(nrow(train)),
testSize =
(nrow(train)+1): nrow(train.data),
virgin=TRUE)
## building containers around the data samples
container = create_container(dtm,
t(training_codes),
trainSize = 1:(nrow(train.data)),
testSize =
(nrow(train)+1): nrow(train.data),
virgin=TRUE)
x <- train
container = create_container(dtm,
t(training_codes),
trainSize = 1:(nrow(x)),
testSize =
(nrow(train)+1): nrow(train.data),
virgin=TRUE)
container = create_container(dtm,
t(training_codes),
trainSize = 1:(nrow(x)),
testSize =
(nrow(train)+1): nrow(train.data),
virgin=TRUE)
length(t(training_codes))
length(training_codes)
container = create_container(dtm,
length(training_codes),
trainSize = 1:(nrow(x)),
testSize =
(nrow(train)+1): nrow(train.data),
virgin=TRUE)
length(dtm)
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
library(dendextend) # for comparing two dendrograms
require(tables)
require(ggplot2)
require(dplyr)
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
setwd("~/ISB-H/term2/practicum")
require(forecast)
require(zoo)
X <- read.csv('olympics.csv')
dim(X)
colnames(X)
str(X)
head(X)
X$Medal <- as.character(X$Medal)
X <- subset(X, Medal!= "NA" )
dim(X)
##find count of missing values
sapply(X, function(x) sum(is.na(x)))
##no of medals by each team
# plot
library(ggplot2)
theme_set(theme_classic())
##no of teams
library(dplyr)
X %>%
group_by(Year) %>%
summarise(n_distinct(Medal))
library("sqldf")
x <- sqldf("SELECT Year,COUNT(ID) as no_of_obs FROM X GROUP BY Year")
x
x1 <- sqldf("SELECT Year,COUNT(Medal) as no_of_medals FROM X GROUP BY Year")
x1 <- as.data.frame(x1)
y_1992 <- X
##find count of missing values
sapply(y_1992, function(x) sum(is.na(x)))
library("cluster")
library("factoextra")
library("magrittr")
library("dplyr")
y_1192_numeric <- select_if(y_1992, is.numeric)
# Replace values with mean: Height
y_1192_numeric$Height[is.na(y_1192_numeric$Height)] <-mean(y_1192_numeric$Height,na.rm=T)
table(is.na(y_1192_numeric$Height ))
y_1192_numeric$Weight[is.na(y_1192_numeric$Weight)] <-mean(y_1192_numeric$Weight,na.rm=T)
table(is.na(y_1192_numeric$Weight ))
y_1192_numeric$Age[is.na(y_1192_numeric$Age)] <-mean(y_1192_numeric$Age,na.rm=T)
table(is.na(y_1192_numeric$Age ))
###
myvars <- c("Age", "Height", "Weight")
newdata <- y_1192_numeric[myvars]
dim(newdata)
# Replace values with mean: Height
newdata$Height[is.na(newdata$Height)] <-mean(newdata$Height,na.rm=T)
table(is.na(newdata$Height ))
my_data <- newdata %>%
na.omit() %>%          # Remove missing values (NA)
scale()
dim(my_data)
my_data <- newdata %>%
na.omit() %>%          # Remove missing values (NA)
scale()
dim(my_data)
my_data <- as.data.frame(my_data)
## Box Plot: Height
boxplot(newdata$Height)
## Quantile values: Height
quantile(newdata$Height,prob=c(0,0.01,0.05,0.25,0.5,0.75,0.95,0.99,1.0))
## Variable value capping: Height
newdata$Height[newdata$Height > 205]
newdata$Height[newdata$Height < 150]
