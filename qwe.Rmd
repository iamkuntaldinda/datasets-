---
title: "QWE"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:


## Including Plots

```{r}
library(readxl)
QWE_Excel_1_ <- read_excel("QWE-Excel (1).xlsx",sheet = "Case Data")

class(QWE_Excel_1_)
```


```{r}
qwe <- QWE_Excel_1_ 

library(mice)
library(nnet)
library(caret)
library(DataExplorer)
#install.packages('scales')
library(scales)
#install.packages('data.table')
library(data.table)
require(ggplot2)
```



```{r}
summary(QWE_Excel_1_)
plot_str(QWE_Excel_1_)  #we have all variable int tye
plot_missing(QWE_Excel_1_) # 5 variables have Missing values
plot_histogram(QWE_Excel_1_)  #to plot histogram of continuous variables
plot_density(QWE_Excel_1_)    #to plot density plot of variable

```



```{r}
plot(prop.table(table(QWE_Excel_1_$`Churn (1 = Yes, 0 = No)`)))
colnames(qwe)[which(names(qwe) == "CHI Score 0-1")] <- "CHI_score"
colnames(qwe)[which(names(qwe) == "Customer Age (in months)")] <- "customer_age"
colnames(qwe)[which(names(qwe) == "Churn (1 = Yes, 0 = No)")] <- "Churn"
colnames(qwe)[which(names(qwe) == "CHI Score Month 0")] <- "churn_score_0"
colnames(qwe)[which(names(qwe) == "Support Cases Month 0")] <- "Support_Cases_0"
colnames(qwe)[which(names(qwe) == "CHI Score 0-1")] <- "CHI_0_1"
colnames(qwe)[which(names(qwe) == "Support Cases 0-1")] <- "Support_Cases_0_1"
colnames(qwe)[which(names(qwe) == "SP Month 0")] <- "sp_mon_0"
colnames(qwe)[which(names(qwe) == "Views 0-1")] <- "views_0_1"
colnames(qwe)[which(names(qwe) == "SP 0-1")] <- "SP_0_1"
colnames(qwe)[which(names(qwe) == "Logins 0-1")] <- "logins_0_1"
colnames(qwe)[which(names(qwe) == "Blog Articles 0-1")] <- "blog_articles_0_1"
colnames(qwe)[which(names(qwe) == "Views 0-1")] <- "views_0_1"
colnames(qwe)[which(names(qwe) == "Days Since Last Login 0-1")] <- "DaysSinceLast_Login_0_1"


```


```{r}

df <- qwe[,c(2,3,4,5,6,7,8,9,10,11,12,13)]

head(df)
df <- as.data.frame(df)

cor(df, method = c("pearson", "kendall", "spearman"))

#install.packages('PerformanceAnalytics')
library("PerformanceAnalytics")

  
qwe$ID -> id_qwe
df$ID <- id_qwe

```


To test that Age is a factor related to Churn rate. We have created 3 levels of age. 
1.	Less than 6 months old -  32.31%
2.	Between 6 and 14 months - 29.96% 
3.	More than 14 months - 37.71%


Number of customers of age more than 14 months churn more. 
It shows that there is no dependency on customer age as claimed by Wall's belief


```{r}
df$customer_age -> age
age <- ifelse((age>=0) & (age<=6),"lessthan6",
               ifelse((age>6) & (age<=14),"6_14",
                      ifelse((age>14),"morethan14","NA")))

plot(prop.table(table(age)))

df$age_level <- age
str(df)

df$age_level <- as.factor(df$age_level)
colnames(df)
str(df)
df$Churn <- as.factor(df$Churn)
ggplot(aes(y = df$customer_age, x = Churn, fill = age_level), data = df) + geom_boxplot()
###no dependence on age 


```

2. Run a single regression model that best predicts the probability that a customer leaves. What is the predicted probability that customer 672 will leave between December 2011 and February 2012? Is that high or low? Did that customer actually leave? What about customers 354 and 5203?
```{r}
# Create Training Data

i_o <- df[which(df$Churn == 1), ]  # all 1's
i_z <- df[which(df$Churn == 0), ]  # all 0's
set.seed(140)  # for repeatability of samples

i_o_t_r <- sample(1:nrow(i_o), 0.75*nrow(i_o))  # 1's for training
i_z_t_r <- sample(1:nrow(i_z), 0.75*nrow(i_o))  # 0's for training. Pick as many 0's as 1's
t_o <- i_o[i_o_t_r, ]  
t_z <- i_z[i_z_t_r, ]
trainingData <- rbind(t_o, t_z)  # row bind the 1's and 0's 


```


```{r}
# Create Test Data
test_ones <- i_o[-i_o_t_r, ]
test_zeros <- i_z[-i_z_t_r, ]
testData <- rbind(test_ones, test_zeros)  # row bind the 1's and 0's 

dim(testData)
dim(trainingData)
table(trainingData$Churn)
table(testData$Churn)

```


```{r}
set.seed(111)
Logistic_regression_model <- glm(Churn ~ churn_score_0 + CHI_score + sp_mon_0+ +Support_Cases_0_1+DaysSinceLast_Login_0_1+views_0_1+logins_0_1+age_level+blog_articles_0_1, data=trainingData, family=binomial(link="logit"))
summary(Logistic_regression_model)
anova(Logistic_regression_model)
colnames(trainingData)
colnames(qwe)
```


```{r}
predicted <- plogis(predict(Logistic_regression_model, testData))  # predicted scores

```

The Actual probabilities of the required  customers are given below. It is low and less than 0.5. No, these customers did not left actually.
customer 354 - 0.4809895
customer 672 - 0.46486432
customer 5203 - 0.32574207

```{r}
library(InformationValue)
optCutOff <- 0.75
```


```{r}

require(MASS)
require(car)
vif(Logistic_regression_model)

##devtools::install_github("InformationValue")
misClassError(testData$Churn, predicted, threshold = optCutOff)

```

```{r}
plotROC(testData$Churn,predicted)
table(testData$Churn)
```

```{r}
Concordance(testData$Churn, predicted)

sensitivity(testData$Churn, predicted, threshold = optCutOff)
#> 0.3089
specificity(testData$Churn, predicted, threshold = optCutOff)

confusionMatrix(testData$Churn, predicted, threshold = optCutOff)


```

```{r}
df[c(354,672,5203),]
class(predicted)
predicted <- as.data.frame(predicted)
colnames(predicted)
trainingData$ID -> id_train
testData$ID -> id_test
predicted$id <-  id_test
View(predicted)

```

```{r}

##factors responsible for customers leaving out
library(magrittr)
QWE_Excel_1_$`Churn (1 = Yes, 0 = No)` <- as.factor(QWE_Excel_1_$`Churn (1 = Yes, 0 = No)`)
QWE_Excel_1_ %>% ggplot(aes(x = QWE_Excel_1_$`Churn (1 = Yes, 0 = No)`, y = QWE_Excel_1_$`Customer Age (in months)`, fill = QWE_Excel_1_$`Churn (1 = Yes, 0 = No)`)) +geom_boxplot()  
QWE_Excel_1_ %>% ggplot(aes(x = QWE_Excel_1_$`Churn (1 = Yes, 0 = No)`, y = QWE_Excel_1_$`CHI Score Month 0`, fill = QWE_Excel_1_$`Churn (1 = Yes, 0 = No)`)) +geom_boxplot()  
QWE_Excel_1_ %>% ggplot(aes(x = QWE_Excel_1_$`Churn (1 = Yes, 0 = No)`, y = QWE_Excel_1_$`CHI Score 0-1`, fill = QWE_Excel_1_$`Churn (1 = Yes, 0 = No)`)) +geom_boxplot()  
QWE_Excel_1_ %>% ggplot(aes(x = QWE_Excel_1_$`Churn (1 = Yes, 0 = No)`, y = QWE_Excel_1_$`Support Cases 0-1`, fill = QWE_Excel_1_$`Churn (1 = Yes, 0 = No)`)) +geom_boxplot()  
QWE_Excel_1_ %>% ggplot(aes(x = QWE_Excel_1_$`Churn (1 = Yes, 0 = No)`, y = QWE_Excel_1_$`SP Month 0`, fill = QWE_Excel_1_$`Churn (1 = Yes, 0 = No)`)) +geom_boxplot()  
QWE_Excel_1_ %>% ggplot(aes(x = QWE_Excel_1_$`Churn (1 = Yes, 0 = No)`, y = QWE_Excel_1_$`Logins 0-1`, fill = QWE_Excel_1_$`Churn (1 = Yes, 0 = No)`)) +geom_boxplot()  
QWE_Excel_1_ %>% ggplot(aes(x = QWE_Excel_1_$`Churn (1 = Yes, 0 = No)`, y = QWE_Excel_1_$`Blog Articles 0-1`, fill = QWE_Excel_1_$`Churn (1 = Yes, 0 = No)`)) +geom_boxplot()  
QWE_Excel_1_ %>% ggplot(aes(x = QWE_Excel_1_$`Churn (1 = Yes, 0 = No)`, y = QWE_Excel_1_$`Days Since Last Login 0-1`, fill = QWE_Excel_1_$`Churn (1 = Yes, 0 = No)`)) +geom_boxplot()  

```
Plot 1 - Longevity in months has very less impact on the churn rate of customers.
Plot 2 - CHI score for month 0 shows that customer who have churned have lower CHI score relative to other customers.
Plot 3 - CHI score for month 0-1 shows that customer who have churned have lower CHI score relative to other customers.
Plot 4 - 
Plot 5 - Those customers who have high average Support Priority. will not churn more frequently.
Plot 6 - Those customers who have churned have lesser number of logins as compared to other customers.
Plot 7 - 
Plot 8 - Days since last login is also a factor responsible for Churn rate of customers. From Boxplot , we can see that the customers who have churned have more days since last login.




3. How sensible is the approach with a single model? Can you suggest a better approach? Provide updated estimates of probabilities that customers 672, 354 and 5,203 will leave.

Lets try out with SVM model
```{r}
###SVM model###
library("e1071")
colnames(testData)
colnames(trainingData)

trainingData$ID -> id_train
testData$ID -> id_test
trainingData <- trainingData[,c(1,2,3,4,5,6,7,8,9,10,11,12)]
testData <- testData[,c(1,2,3,4,5,6,7,8,9,10,11,12)]
svm_model <- svm(as.numeric(as.character(Churn)) ~ ., data=trainingData)

class(svm_model)

```


```{r}

pred <- plogis(predict(svm_model,testData))

```


```{r}

#354 - 0.744231670
#672 - 0.622208856
#5203 - 0.18854133
optCutOff <- .75

misClassError(testData$Churn, pred, threshold = optCutOff)

str(pred)
plotROC(as.numeric(as.character(testData$Churn)),as.numeric(pred))

Concordance(testData$Churn, pred)
sensitivity(testData$Churn, pred, threshold = optCutOff)
# 0.50
specificity(testData$Churn, pred, threshold = optCutOff)
#0.822
confusionMatrix(testData$Churn, pred, threshold = optCutOff)
View(pred)

```



Below are the given probabilities for the customers. 
customer 354 - 0.6098852
customer 672 - 0.5867321
customer 5203 - 0.5405019

This time, we have implemented SVM model which means that a kernel trick is used to transformation and then based on these transformations, it finds an optimal boundary between the possible outputs. 
This technique is used for the customers to find an optimal hyperplane to classify the churned customers and non- churn customers. 

These customers have higher probability to churn using SVM model  but as per the cutoff , it is still lesser than 0.75

```{r}
#install.packages('xgboost')
#install.packages('tidyverse')

library(xgboost) # for xgboost
library(tidyverse) # general utility functions
# put our testing & training data into two seperates Dmatrixs objects
train_data <- as.matrix(trainingData)
str(train_data)
colnames(trainingData)

t_d_removed <- trainingData %>%
    select(-starts_with("Churn"))


test_d_removed <- testData %>%
    select(-starts_with("Churn"))


# select just the numeric columns
df_num <- df_removed %>%
    select(-ID) %>% # the case id shouldn't contain useful information
    select_if(is.numeric)


t_d_matrix <- data.matrix(t_d_removed)
test_d_matrix <- data.matrix(test_d_removed)
trainingData$Churn-> labels_churn
test_labels <- testData$Churn
train_labels <- labels_churn
class(train_labels)
class(test_labels)
test_labels <- as.numeric(as.character(test_labels))
train_labels <- as.numeric(as.character(train_labels))

length(labels_churn)
# get the numb 70/30 training test split
numberOfTrainingSamples <- round(length(labels_churn) * .7)

# training data
train_data <- df_matrix[1:numberOfTrainingSamples,]
train_labels <- labels_churn[1:numberOfTrainingSamples]

# testing data
test_data <- df_matrix[-(1:numberOfTrainingSamples),]
test_labels <- labels_churn[-(1:numberOfTrainingSamples)]
prop.table(table(test_labels))
prop.table(table(train_labels))
# put our testing & training data into two seperates Dmatrixs objects
dtrain <- xgb.DMatrix(data = t_d_matrix, label= train_labels)
dtest <- xgb.DMatrix(data = test_d_matrix, label= test_labels)
```


```{r}


# train a model using our training data
model_xg <- xgboost(data = dtrain, # the data   
                 nround = 4, # max number of boosting iterations
                 objective = "binary:logistic")  # the objective function
```

```{r}

# generate predictions for our held-out testing data
pred_xg <- predict(model_xg, dtest)
View(pred_xg)
# get & print the classification error
err <- mean(as.numeric(pred_xg > 0.7) != test_labels)
print(paste("test-error=", err))
View(testData$Churn)


##devtools::install_github("InformationValue")
library(InformationValue)
misClassError(testData$Churn, pred_xg, threshold = 0.75)
plotROC(as.numeric(as.character(testData$Churn)),as.numeric(pred_xg))

sensitivity(testData$Churn, pred_xg, threshold = 0.75)
# 0.222
specificity(testData$Churn, pred_xg, threshold = 0.75)
#0.959
confusionMatrix(testData$Churn, pred_xg , threshold = 0.75)
pred_xg <- as.data.frame(pred_xg)
pred_xg$id <- id_test
View(pred_xg)
```


Below are the given probabilities for the customers. They are lower than cutoffs
customer 354 - 0.6192521
customer 672 - 0.6297252
customer 5203 - 0.4294627

```{r}
# train an xgboost model
model_tuned <- xgboost(data = dtrain, # the data           
                 max.depth = 3, # the maximum depth of each decision tree
                 nround = 2, # max number of boosting iterations
                 objective = "binary:logistic") # the objective function 

# generate predictions for our held-out testing data
pred <- predict(model_tuned, dtest)

# get & print the classification error
err <- mean(as.numeric(pred > 0.7) != test_labels)
print(paste("test-error=", err))

# plot them features! what's contributing most to our model?
install.packages('DiagrammeR')
require(DiagrammeR)
xgb.plot.multi.trees(feature_names = names(t_d_matrix), 
                     model = model_xg)
```
```{r}
# convert log odds to probability
odds_to_probs <- function(odds){
    return(exp(odds)/ (1 + exp(odds)))
}


# probability of leaf above countryPortugul
odds_to_probs(-0.599)

# get information on how important each feature is
importance_matrix <- xgb.importance(names(t_d_matrix), model = model_xg)

# and plot it!
xgb.plot.importance(importance_matrix)


```


#install.packages('DMwR')
library(DMwR)
trainingData$Churn <- as.factor(trainingData$Churn)
trainSplit <- SMOTE(Churn ~ ., trainingData, perc.over = 100, perc.under=200)
trainSplit$Churn<- as.numeric(trainSplit$Churn)
str(trainSplit)
dim(trainSplit)
table(trainSplit$Churn)

table(trainingData$Churn)

```



```{r}


```



```{r}


```



```{r}


```



```{r}


```



```{r}


```



```{r}


```



```{r}


```



```{r}


```



```{r}


```



```{r}


```



```{r}


```


