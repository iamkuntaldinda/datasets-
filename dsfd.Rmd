---
title: "SA-2 GROUP ASSIGNMENT"
Author: "Vineet Kapoor"- 11910076, Balaji Venktesh -119100 , Gireesh Sundaram - 119100
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:


```{r }
library(readxl)
library(MASS)
#install.packages('het.test')
```

```{r }
require(het.test)
df1<- read_excel("Dataset.xls")
df2<- df1
```



```{r }

##sample
set.seed(312)

data=df1[sample(nrow(df1), 1000),]

```

```{r}
head(data)
```

```{r}
colnames(data)

str(data)

sapply(data, function(x) sum(is.na(x)))

```

cleaning data

```{r }

require(dplyr)

#missing values

sapply(data, function(x) sum(is.na(x)))

df <- data %>%mutate(INCOME = ifelse(is.na(INCOME),0, INCOME))

#changed name of two columns AREA(Agrl), Gender(F:Female)

names(df)[names(df) == "AREA(Agrl)"] <- "Area"

names(df)[names(df) == "Gender(F:Female)"] <- "Gender"



```



```{r }
df <- df %>% mutate(Area = ifelse(is.na(Area),0, Area))

sapply(df, function(x) sum(is.na(x)))

head(df)


```

changing datatypes

```{r }


df$Gender <- as.factor(df$Gender)
df$AGE <- as.numeric(df$AGE)
df$CASTE <- as.factor(df$CASTE)
df$RELIGN <- as.factor(df$RELIGN)
df$MTONGUE <- as.factor(df$MTONGUE)
df$OCCU <- as.factor(df$OCCU)


```


VISUALIZING
```{r }



library(ggplot2)

g <- ggplot(df, aes(Gender,TOTAL))

g + geom_bar(stat = "identity",fill = "blue") +coord_flip() 

g <- ggplot(df, aes(CASTE,TOTAL))

g + geom_bar(stat = "identity",fill = "blue") +coord_flip() 

g <- ggplot(df, aes(RELIGN,TOTAL))

g + geom_bar(stat = "identity",fill = "blue") +coord_flip() 

g <- ggplot(df, aes(MTONGUE,TOTAL))

g + geom_bar(stat = "identity",fill = "blue") +  coord_flip() 

g <- ggplot(df, aes(OCCU,TOTAL))

g + geom_bar(stat = "identity",fill = "blue") +coord_flip() 




```

```{r }
qplot(WRITE, data=df, geom="density", fill=Gender, alpha=I(.5)) ##normal

qplot(READ, data=df, geom="density", fill=Gender, alpha=I(.5))  ##left skew

qplot(MATH, data=df, geom="density", fill=Gender, alpha=I(.5))  ##left skew for female

qplot(TOTAL, data=df, geom="density", fill=Gender, alpha=I(.5))  ##left skew

qplot(TOTAL, data=df, geom="density", alpha=I(.5))  ##left skew

```


boxplot
```{r }

boxplot(TOTAL~Gender,data=df)

df$TOTAL[df$TOTAL< 23] <- 23

#df$TOTAL[df$TOTAL> 90] <- 90

boxplot(INCOME~Gender,data=df)

boxplot(Area~Gender,data=df)

boxplot(AGE~Gender,data=df)


boxplot(TOTAL~Gender,data=df)


```

Outlier Treatment

```{r }

df$AGE[df$AGE >= 50]  <- NA

df <- df %>%na.omit()

quantile(df$INCOME,0.95)
df$INCOME[df$INCOME > 12000] <- 12000
boxplot(INCOME~Gender,data=df)
###
```


Correlation matrix
```{r }
####correlation matrix

M<-df[,8:13]

M$AGE<- df$AGE

c <- cor(M)

head(round(c,2))
#install.packages('corrplot')
require(corrplot)


corrplot::corrplot(c,method = "color")
```

creating dummy variables

```{r }

#install.packages("fastDummies")

require(fastDummies)

df <- fastDummies::dummy_cols(df, select_columns = "Gender")

df <- fastDummies::dummy_cols(df, select_columns = "OCCU")

df <- fastDummies::dummy_cols(df, select_columns = "RELIGN")

df <- fastDummies::dummy_cols(df, select_columns = "CASTE")

df <- fastDummies::dummy_cols(df, select_columns = "MTONGUE")


#removing dummy columns from each of the category
df$Gender_T <- NULL
df$OCCU_T <-  NULL
df$RELIGN_M <- NULL
df$CASTE_ST <- NULL
df$MTONGUE_H<- NULL

write.csv(df,'sampleset.csv')


```


backward using regsubsets
```{r }

require(leaps)
reg2 = regsubsets(TOTAL~AGE+INCOME+Gender_F+OCCU_C+OCCU_A+OCCU_H+OCCU_B+OCCU_U+RELIGN_H+RELIGN_C + CASTE_SC +CASTE_OT+MTONGUE_T+MTONGUE_K+MTONGUE_U+MTONGUE_D,data = df,nbest=10,method = "backward")

summary(reg2)

plot(reg2,scale="r2")


```

```{r }
require(leaps)

reg <- regsubsets(TOTAL~AGE+INCOME+Gender_F+OCCU_C+OCCU_A+OCCU_H+OCCU_B+OCCU_U+RELIGN_H+RELIGN_C + CASTE_SC +CASTE_OT+MTONGUE_T+MTONGUE_K+MTONGUE_U+MTONGUE_D,data = df, nbest=10)

summary(reg)

```


randomly modelling
```{r }

full_model <- lm(TOTAL ~. ,data = df)
summary(full_model)
```



####randomly modelling

```{r }
m3 <- lm(TOTAL ~ AGE:Gender_F:CASTE_SC + CASTE_SC  + MTONGUE_K:OCCU_H +RELIGN_H:OCCU_B:AGE+MTONGUE_U+ CASTE_OT+ Gender_F+ CASTE_OT:Gender_F+ OCCU_U:MTONGUE_U  + OCCU_C:MTONGUE_U:CASTE_SC ,data = df)
summary(m3)

##Intercept of 43.25 is the mean total score of students who are Males, have mtongue as  H, occupation as T, religion as M, caste as ST
###Effect of  Mtongue_K and Occu_H interact, change in MTONGUE_K effect if someone has occupation of H and it is 27.70 times.
###Total score will increase by 7.3082 if there is 1 unit increase in count of female.
```
##model interpretation



Partial F-test.
It shows that reduced model and full model are not same and there is decrease in RSS in huge model and it is significant.

```{r}
anova(full_model,m3)
```


```{r }
plot(m3)

```



```{r }
#Other useful functions 
coefficients(m3) # model coefficients
confint(m3, level=0.95) # CIs for model parameters 
anova(m3) # anova table 
vcov(m3) # covariance matrix for model parameters 
```


Standardize

```{r }
#install.packages('lm.beta')
library(lm.beta)
lm.std.beta <- lm.beta(m3)
summary(lm.std.beta)

```

```{r}
print(lm.std.beta,standardized=T)

```


```{r }
#95% confidence interval for Beta coefficients

options(scipen = 999)

confint(m3,level=0.95)

```

```{r }
#QQ-plot of the residuals

qqnorm(rstandard(m3))

qqline(rstandard(m3))

```


vif test
Caste_sc , Gender_F:CASTE_OT, CASTE_OT and Gender_F  have vif greater than 10 , it indicates multicollinearity between them. and variation in total score is 25.15 times higher than when it is taken alone in model.

```{r }
vif(m3)

```



```{r }
#install.packages(c("olsrr", "v0.5.1"))

library(olsrr,v0.5.1)

k=ols_step_best_subset(m3,method="AIC")

plot(k)

```



```{r }
#forward selection method

i <- ols_step_forward_p(m3)

plot(i)

```



```{r }

#backward selection method

j<-ols_step_backward_p(m3)

```


Cook's Distance:

```{r }
cooksd <- cooks.distance(m3)

cooksd

```



```{r }
# plot cook's distance

plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance") 


```




```{r }
# influential row numbers

influential <- names(cooksd)[(cooksd > 4*mean(cooksd, na.rm=T))]

influential

```



```{r }
#DFBETAs

dfbetas=dfbetas(m3)

dfbetas

ols_plot_dfbetas(m3)

```

```{r }
#DFFITs

ols_plot_dffits(m3)

dffits=dffits(m3)

dffits

```



```{r }
#Studentized residual Plot

ols_plot_resid_stud(m3)
```


```{r }


#Outlier & Leverage

ols_plot_resid_lev(m3)

```


DurbinWatson test
###the value of statistic is  in normal range, and the we fail to reject the null hypothesis that there is slightly negative autocorrelation.
```{r }
car::durbinWatsonTest(m3)
```



```{r }
#stepwise regression
fit_Step <- lm(TOTAL~AGE+INCOME+Gender_F+OCCU_C+OCCU_A+OCCU_H+OCCU_B+OCCU_U+RELIGN_H+RELIGN_C + CASTE_SC +CASTE_OT+MTONGUE_T+MTONGUE_K+MTONGUE_U+MTONGUE_D,data=df)

step <- stepAIC(fit_Step, direction="both")
step$anova # display results

```



```{r }
#backward elimination using stepaic

Model1=lm(TOTAL~AGE+INCOME+Gender_F+OCCU_C+OCCU_A+OCCU_H+OCCU_B+OCCU_U+RELIGN_H+RELIGN_C + CASTE_SC +CASTE_OT+MTONGUE_T+MTONGUE_K+MTONGUE_U+MTONGUE_D,data=df)

step <- stepAIC(Model1, direction="back")

```









Question -2 
Logistic regression using same dataset
```{r }

head(df)
dim(df)

```


###Creating new performance variable
```{r }
df$performance[df$TOTAL > 70] <- 1
df$performance[df$TOTAL <= 70] <- 0
dim(df)

```


Using almost 70% the data for sampling - training and testing samples.
```{r }
train <- df[1:700,]
test <- df[701:992,]

```


seeing the class and checking if there is class imbalance or not
```{r }
table(train$performance)
table(test$performance)
```


```{r }

train <- as.data.frame(train)
test <- as.data.frame(test)
```

Creating first model
```{r }

model1 <- glm(performance ~ AGE:Gender_F:CASTE_SC + CASTE_SC  + MTONGUE_K:OCCU_H +RELIGN_H:OCCU_B:AGE+MTONGUE_U+ CASTE_OT+ Gender_F+ CASTE_OT:Gender_F+ OCCU_U:MTONGUE_U  + OCCU_C:MTONGUE_U:CASTE_SC  ,data=train, family = binomial)
summary(model1)

```

```{r }

model2 <- glm(performance ~ CASTE_SC + OCCU_C + MTONGUE_U + OCCU_H:MTONGUE_K + OCCU_U:MTONGUE_U  + OCCU_C:MTONGUE_U:CASTE_SC  ,data=train, family = binomial)
summary(model2)

```


Third model
```{r }
model3 <- glm(performance ~  CASTE_OT  + MTONGUE_T  + OCCU_C:MTONGUE_U:CASTE_SC  ,data=train, family = binomial)
summary(model3)

```



For model 1 , 
p-value is less than 0.05 and it is significant, so reject null hypothesis and this model is goo for prediction as it has got least Deviance Residual
```{r}
1-pchisq(641.40-621.34,699-689)
```

for model 2
```{r}
1-pchisq(641.40-626.51,699-693)
```

Visualisation of variables with target variable
```{r }
g <- ggplot(df, aes(performance,TOTAL))
g + geom_bar(stat = "identity",aes(fill = Gender)) +coord_flip() 

g <- ggplot(df, aes(performance,TOTAL))
g + geom_bar(stat = "identity",aes(fill = CASTE)) +coord_flip() 

g <- ggplot(df, aes(performance,TOTAL))
g + geom_bar(stat = "identity",aes(fill = OCCU)) +coord_flip() 


```

Predicting values using model1, class 0 - 236 values, Class 1  - 56 values
```{r }
p <- predict(model1,test, type = "response")
fit <- ifelse(p > 0.20,1,0)
table(fit)

```


Variables Importance and their estimates can be find out using this.
```{r }
library(broom)
tidy(model1)

```

McFladden R-square shows that the R-square values of the 3 diffrent models are vary low and model1 has 0.03 R- square
```{r }
require(pscl)
list(MODEL1 = pscl::pR2(model1)["McFadden"],
     MODEL2 = pscl::pR2(model2)["McFadden"],
     MODEL3 = pscl::pR2(model3)["McFadden"])
```

ROC curve for the model1 and it is the plot between True positive rate and False positive rate.AUC of this curve is near to 0.5 
```{r }
library(pROC)
roccurve <- roc(test$performance ~ p)
plot(roccurve)
```


Hosmer - lemeshow test shows that the model is logistic regression model as Null hypothesis shows that model follows Logistic regression model...
```{r }
library(generalhoslem)
logitgof(test$performance, p)
```

Confusion matrix shows that 15% of the observation fall in Type -1 error and 
13 % observations are in Type -2 error. 
only 4% values predicted are high achievers and 67% are true negative values means low achievers. 
```{r }
c = table(test$performance, p > 0.20) %>% prop.table()
c
```

```{r }

```

